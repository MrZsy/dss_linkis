**先决条件**：
1. 安装机上需要有cdh6.3.2的环境，cdh环境中还需要有flink环境。
2. docker环境。

参考文章：

[CM+CDH6.3.2环境搭建](https://www.cnblogs.com/ttzzyy/p/13072774.html)
   
[flink编译parcel包并集成至cdh6.3.2](https://blog.csdn.net/cn987654/article/details/117516124?spm=1001.2101.3001.6650.17&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-17-117516124-blog-123007848.pc_relevant_aa&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-17-117516124-blog-123007848.pc_relevant_aa&utm_relevant_index=22)
   
[Docker安装教程](https://www.jianshu.com/p/946e703df75d?v=1678933661213)

以上环境没有的同学可以参考上面的文章安装或者自行百度安装。


1、 版本说明：
1. WeDataSphere组件版本：

| 组件名 | Apache Linkis | DataSphere Studio | Schedulis | Qualitis | Exchangis | Visualis | Streamis | MYSQL  | JDK |
|-------|---------------|-------------------|-----------|----------|-----------|----------|----------|--------|-----|
| 版本号 | 1.3.1         | 1.1.1             | 0.7.1     | 0.9.2    | 1.0.0     | 1.0.0    | 0.2.0    | 5.1.49 | 1.8 |


2. CDH6.3.2 版本

| 组件名 | Hadoop         | Hive           | Spark | Flink  | Sqoop | Trino |
|-------|----------------|----------------|-------|--------|-------|-------|
| 版本号 | 3.0.0-cdh6.3.2 | 2.1.1-cdh6.3.2 | 3.0.0 | 1.12.4 | 1.4.6 | 371   |

2、github 拉取代码

```
git clone https://github.com/MrZsy/dss_linkis.git
```
3、修改conf 文件，配置自己的cdh中hive元数据的地址

cd docker/conf/ 目录下：
![image.png](https://cdn.jsdelivr.net/gh/MrZsy/noteImage@main/img/202303161054156.png)

找到conf.ini文件，修改成自己cdh中hive元数据的数据库信息
![image.png](https://cdn.jsdelivr.net/gh/MrZsy/noteImage@main/img/202303161045541.png)
4、构建镜像

cd 到项目根目录下执行构建
```
docker build -t dss_linkis_server .
```
执行后等待构建成功即可。

5、修改docker 启动文件 

![image.png](https://cdn.jsdelivr.net/gh/MrZsy/noteImage@main/img/202303161056381.png)
vim run_docker_dss_server.sh
```
# 启动容器
docker run  \
# 容器名称 
--name dss_linkis_server_v1 \
# 容器hostname 固定不用改动
-h dss.node.cn \
# 设置环境变量，需要设置成自己的宿主机ip
-e HOST_IP=172.16.13.133 \
# 目录挂载，需要修改成自己机器上的路径
# 本地cdh环境挂载到容器中
-v /opt/cloudera/parcels/CDH:/wedatasphere/cdh \
# 本地spark3挂载到容器中
-v /usr/local/service/spark3:/wedatasphere/service/spark3 \
# 本地flink挂载到容器中
-v /opt/cloudera/parcels/FLINK:/wedatasphere/service/flink \
# 本地hadoop相关conf目录挂载
-v /etc/hadoop/conf:/wedatasphere/cdh/config/hadoop \
-v /etc/sqoop/conf:/wedatasphere/cdh/config/sqoop \
-v /etc/hive/conf:/wedatasphere/cdh/config/hive \
-v /etc/hadoop/conf:/etc/hadoop/conf \
-v /etc/sqoop/conf:/etc/sqoop/conf \
-v /etc/hive/conf:/etc/hive/conf \
# 端口映射
-p 3306:3306 \
-p 8085:8085 \
-p 9600:9600 \
-p 8060:8060 \
-p 8090:8090 \
-p 9098:9098 \
-p 8055:8055 \
-itd \
# 上步构建时用的image名称
dss_linkis_server
```
6、运行docker
```
sh run_docker_dss_server.sh 
```

7、查看是否运行成功

![image.png](https://cdn.jsdelivr.net/gh/MrZsy/noteImage@main/img/202303161133229.png)

查看docker日志看各服务是否启动
```
docker logs  CONTAINER_ID -f 
```
8、登录

浏览器访问地址 http://宿主机IP:8085/#/login 即可登录系统UI。
默认的用户名和密码是:hdfs/hdfs
![image.png](https://cdn.jsdelivr.net/gh/MrZsy/noteImage@main/img/202303161139967.png)

